{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 27 June 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station inforamtion to MetaboLights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftplib import FTP\n",
    "#from openpyxl import Workbook\n",
    "#pd.options.mode.copy_on_write = True #will be the default, may as well set it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45ac813-b5f5-4314-834c-5c8e6c901b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the TSQ data (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS2356'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0217b1-d7f9-491d-9284-f6f0f7edaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftp.cwd('/pub/databases/metabolights/studies/public/' + study_id)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640a543f-aadf-4eda-99ca-62e766f6c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FILES',\n",
       " 'HASHES',\n",
       " 'METADATA_REVISIONS',\n",
       " 'a_MTBLS2356_LC-MS_negative__metabolite_profiling.txt',\n",
       " 'a_MTBLS2356_LC-MS_positive__metabolite_profiling.txt',\n",
       " 'i_Investigation.txt',\n",
       " 'm_MTBLS2356_LC-MS_negative__metabolite_profiling_v2_maf.tsv',\n",
       " 'm_MTBLS2356_LC-MS_positive__metabolite_profiling_v2_maf.tsv',\n",
       " 's_MTBLS2356.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b281188c-0bbc-408f-8b88-a3be2e167abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata information so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "\n",
    "# metadataFiles\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "    \n",
    "#ftp.quit()  #can close this down now as I have the files I need (careful when I get to the loop\n",
    "#print(\"FTP closed\")\n",
    "\n",
    "metadata = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aeae2d7-ffdf-4b52-9fd6-9fd8c29dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull Source Name as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata['Source Name']\n",
    "depth = metadata['Factor Value[Depth]']\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29c8922-5b7b-4017-bcfb-9559e461a282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2016-07-09T18:04:00\n",
       "1    2016-07-09T18:04:00\n",
       "2    2016-07-09T18:04:00\n",
       "3    2016-07-10T00:02:00\n",
       "4    2016-07-10T00:02:00\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "date_cmap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "### Need BIOS-SCOPE file for lat/lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988bd0e6-09e1-4813-92bb-989fcf659da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "fName = 'data/BATS_BS_COMBINED_MASTER_latest.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a245c8b-a8a4-4582-981b-a41bf8b9bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb95e43-079e-4082-8e1f-b2fb86164636",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00364018-f720-4c0e-8058-8d4c23772743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35ed1a2-bc00-44f9-9245-0775f37afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 #make a loop later as can have multiple data files for a single dataset\n",
    "writeFile = 'data/' + 'tempData.tsv'\n",
    "readFile = dataFiles.loc[0,'files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db77f16-d182-4a71-b444-9f86e3ebc0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "        \n",
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bbbe2a0-5b3e-4e52-9833-6926771daa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the test, can I read in the resulting file? YES, so this is the datafile\n",
    "tsvFile = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8396f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of    database_identifier chemical_formula  \\\n",
       "0          CHEBI:18026           C7H6O4   \n",
       "1          CHEBI:77267          C3H8O5S   \n",
       "2          CHEBI:44111          C3H6O2S   \n",
       "3          CHEBI:30763           C7H6O3   \n",
       "4          CHEBI:27744         C3H8NO5P   \n",
       "5          CHEBI:16750       C10H13N5O5   \n",
       "6          CHEBI:17596       C10H12N4O5   \n",
       "7           CHEBI:6650           C4H6O5   \n",
       "8          CHEBI:17533         C7H11NO5   \n",
       "9          CHEBI:47571        C11H19NO8   \n",
       "10         CHEBI:68329          C9H10O5   \n",
       "11         CHEBI:28865       C26H45NO7S   \n",
       "12         CHEBI:17748       C10H14N2O5   \n",
       "13         CHEBI:18107       C10H12N4O6   \n",
       "\n",
       "                                               smiles  \\\n",
       "0                                  OC(=O)c1cccc(O)c1O   \n",
       "1                                   OCC(O)CS(O)(=O)=O   \n",
       "2                                           OC(=O)CCS   \n",
       "3                                   OC(=O)c1ccc(O)cc1   \n",
       "4                                  OC(=O)CNCP(O)(O)=O   \n",
       "5   Nc1nc2n(cnc2c(=O)[nH]1)[C@@H]1O[C@H](CO)[C@@H]...   \n",
       "6    OC[C@H]1O[C@H]([C@H](O)[C@@H]1O)n1cnc2c(O)ncnc12   \n",
       "7                                   OC(CC(O)=O)C(O)=O   \n",
       "8                       CC(=O)N[C@@H](CCC(O)=O)C(O)=O   \n",
       "9   C[C@@H](O[C@H]1[C@H](O)[C@@H](CO)O[C@H](O)[C@@...   \n",
       "10                            COc1cc(cc(OC)c1O)C(O)=O   \n",
       "11  [H][C@@]12C[C@H](O)CC[C@]1(C)[C@@]1([H])C[C@H]...   \n",
       "12    Cc1cn([C@H]2C[C@H](O)[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
       "13  OC[C@H]1O[C@H]([C@H](O)[C@@H]1O)n1cnc2c1[nH]c(...   \n",
       "\n",
       "                                                inchi  \\\n",
       "0   InChI=1S/C7H6O4/c8-5-3-1-2-4(6(5)9)7(10)11/h1-...   \n",
       "1   InChI=1S/C3H8O5S/c4-1-3(5)2-9(6,7)8/h3-5H,1-2H...   \n",
       "2     InChI=1S/C3H6O2S/c4-3(5)1-2-6/h6H,1-2H2,(H,4,5)   \n",
       "3   InChI=1S/C7H6O3/c8-6-3-1-5(2-4-6)7(9)10/h1-4,8...   \n",
       "4   InChI=1S/C3H8NO5P/c5-3(6)1-4-2-10(7,8)9/h4H,1-...   \n",
       "5   InChI=1S/C10H13N5O5/c11-10-13-7-4(8(19)14-10)1...   \n",
       "6   InChI=1S/C10H12N4O5/c15-1-4-6(16)7(17)10(19-4)...   \n",
       "7   InChI=1S/C4H6O5/c5-2(4(8)9)1-3(6)7/h2,5H,1H2,(...   \n",
       "8   InChI=1S/C7H11NO5/c1-4(9)8-5(7(12)13)2-3-6(10)...   \n",
       "9   InChI=1S/C11H19NO8/c1-4(10(16)17)19-9-7(12-5(2...   \n",
       "10  InChI=1S/C9H10O5/c1-13-6-3-5(9(11)12)4-7(14-2)...   \n",
       "11  InChI=1S/C26H45NO7S/c1-15(4-7-23(31)27-10-11-3...   \n",
       "12  InChI=1S/C10H14N2O5/c1-5-3-12(10(16)11-9(5)15)...   \n",
       "13  InChI=1S/C10H12N4O6/c15-1-3-5(16)6(17)9(20-3)1...   \n",
       "\n",
       "           metabolite_identification  mass_to_charge  fragmentation  \\\n",
       "0          2,3-dihydroxybenzoic acid             NaN            NaN   \n",
       "1   2,3-dihydroxypropane-1-sulfonate             NaN            NaN   \n",
       "2              3-mercaptoproprionate             NaN            NaN   \n",
       "3              4-hydroxybenzoic acid             NaN            NaN   \n",
       "4                         glyphosate             NaN            NaN   \n",
       "5                          guanosine             NaN            NaN   \n",
       "6                            inosine             NaN            NaN   \n",
       "7                         malic acid             NaN            NaN   \n",
       "8             n-acetyl glutamic acid             NaN            NaN   \n",
       "9              n-acetyl muramic acid             NaN            NaN   \n",
       "10                     syringic acid             NaN            NaN   \n",
       "11                      taurocholate             NaN            NaN   \n",
       "12                         thymidine             NaN            NaN   \n",
       "13                        xanthosine             NaN            NaN   \n",
       "\n",
       "    modifications  charge  retention_time  ...  s1035700708  s1035700712  \\\n",
       "0             NaN     NaN             NaN  ...     0.312137     0.000000   \n",
       "1             NaN     NaN             NaN  ...     0.000000    55.058564   \n",
       "2             NaN     NaN             NaN  ...     0.000000     0.000000   \n",
       "3             NaN     NaN             NaN  ...    27.762686    51.528715   \n",
       "4             NaN     NaN             NaN  ...     0.000000     0.000000   \n",
       "5             NaN     NaN             NaN  ...    10.721416     0.000000   \n",
       "6             NaN     NaN             NaN  ...     0.000000     1.915867   \n",
       "7             NaN     NaN             NaN  ...     0.000000     0.000000   \n",
       "8             NaN     NaN             NaN  ...    32.034626     0.000000   \n",
       "9             NaN     NaN             NaN  ...     0.000000     0.000000   \n",
       "10            NaN     NaN             NaN  ...     0.000000    12.179295   \n",
       "11            NaN     NaN             NaN  ...     0.916287     0.000000   \n",
       "12            NaN     NaN             NaN  ...     0.000000     0.000000   \n",
       "13            NaN     NaN             NaN  ...     6.209146     0.000000   \n",
       "\n",
       "    s1035700704  s1035700718 s1035802401  s1035802424  s1035802408  \\\n",
       "0      0.445780     0.000000    0.000000     0.000000     0.326205   \n",
       "1      0.000000     0.000000    0.000000     0.000000   105.662386   \n",
       "2      0.000000    31.435391   58.970583    13.451468    29.714441   \n",
       "3     97.321794     0.000000    0.000000     0.000000    34.718597   \n",
       "4      0.000000     0.000000    0.000000     0.000000     0.000000   \n",
       "5      0.000000     0.000000    0.000000     0.000000     0.000000   \n",
       "6      0.000000     0.000000    0.000000     0.000000     0.000000   \n",
       "7     19.972436     0.000000   62.009964     0.000000     8.813383   \n",
       "8      0.000000     0.000000    0.000000     0.000000     0.000000   \n",
       "9      0.000000   795.915003  496.589551     0.000000   136.656880   \n",
       "10     0.000000     0.000000    0.000000     0.000000     6.763457   \n",
       "11     0.000000     0.000000    0.999850     0.000000     0.000000   \n",
       "12     0.000000    15.751638    0.000000     0.000000    63.416425   \n",
       "13    10.453771     5.394521   13.197771     0.000000     0.000000   \n",
       "\n",
       "    s1035802412  s1035802404  s1035802418  \n",
       "0      0.000000     0.561839     0.245316  \n",
       "1    109.733958     0.000000     0.000000  \n",
       "2     20.335266     0.000000    17.968961  \n",
       "3      0.000000    28.922005     0.000000  \n",
       "4      0.000000     0.000000     0.000000  \n",
       "5      0.000000     0.000000     0.000000  \n",
       "6      0.000000     0.000000     0.000000  \n",
       "7      0.000000    64.302920     0.000000  \n",
       "8      0.000000    14.984255     0.000000  \n",
       "9      0.000000     0.000000     0.000000  \n",
       "10    40.662588    11.778122     6.964044  \n",
       "11     0.000000     1.172640     0.000000  \n",
       "12     0.000000     0.000000     0.000000  \n",
       "13     0.000000    12.854771     2.436146  \n",
       "\n",
       "[14 rows x 393 columns]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvFile.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information, will also use this for the sheet with metadata about the variables\n",
    "nVariables = len(dataOnly.columns) #need this for the sheet for the metadata on the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start assembling into CMAP format\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2b16a-8361-49d1-a8e5-6e21e59ae5e3",
   "metadata": {},
   "source": [
    "metadata about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "fName = 'datasetTemplate.xlsx'\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(0,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68e818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to details about the metabolites\n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "df2['var_short_name'] = dataOnly.columns\n",
    "df2.loc[:,'var_long_name'] = tsvFile.loc[:,'metabolite_identification']\n",
    "df2.loc[:,'var_sensor'] = 'Triple quadrupole mass spectrometer (TSQ Vantage, Thermo Scientific)'\n",
    "df2.loc[:,'var_unit'] = 'pM' #this is in the protocols, but I also have some inside information here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "df2.loc[:,('var_discipline')] = 'chemistry'\n",
    "df2.loc[:,('var_visualize')] = 1 #yes/no, all metabolites can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8440118-4769-4e8a-885b-44de2f324d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'var_keywords' will be the hardest as the metabolites have many, many keywords. \n",
    "#I want to talk to the people at CMAP about best options to handle that (start a list of topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9eb734-e142-4c3d-b258-a354df9d8b9a",
   "metadata": {},
   "source": [
    "metadata about the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568a481-d5f6-4ade-889b-e2303afdcba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86bd45-f3f4-4069-b6a0-fd9c21e71b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update this ... can use information from MetaboLights (later)\n",
    "\n",
    "# finally gather up the dataset_meta_data\n",
    "# assemble the details here, might setup in a separate text file later\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "    'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': ['2025-06-25'],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': ['This dataset includes analyses from Niskin bottle samples collected on R/V Atlantic Explorer cruises as part of the BIOS-SCOPE campaign in the time period from 2016 until 2025. Included are CTD data, and survey biogeochemical samples including inorganic nutrients, particulate organic carbon and nitrogen, dissolved organic carbon, dissolved organic nitrogen, total dissolved amino acids, bacterial abundance and production.'],\n",
    "    'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "    'climatology': [0]\n",
    "    })\n",
    "\n",
    "#get the list of cruise names from the bcodmo data file\n",
    "t = pd.DataFrame(bcodmo['Cruise_ID'].unique())\n",
    "t.columns = ['cruise_names']\n",
    "df3 = pd.concat([df3,t],axis=1,ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/forCMAPfromMetabolights.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233c6-73ef-4e1a-87e8-e9b627977a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a189960-4354-4b44-85b7-f17a2eaa18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise UserWarning('Stopping and leave code below for historical reasons, code will not run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca46aa-613d-4b3a-a480-11fd7a4bd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I can get isatools to install, this will be an easier way (I think/hope) to access data at BCO-DMO. There is also an R equivalent at metabolighteR that provides access to MetaboLights REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a526e-326d-408d-8e41-ac117cfcfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install isatools #fails with this error:AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
    "pip install git+https://github.com/ISA-tools/isa-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f991bc-f47e-4cf6-8670-f856c0764fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71df46-62b2-44a2-8330-b2b58c2b02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isatools.net import mtbls as MTBLS\n",
    "tmp_dir = MTBLS.get('MTBLS2356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdef19-fe01-412b-8b46-15ef0fdaf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
