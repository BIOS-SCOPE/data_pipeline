{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 27 June 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station inforamtion to MetaboLights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftplib import FTP\n",
    "#from openpyxl import Workbook\n",
    "pd.options.mode.copy_on_write = True #will be the default, may as well set it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d45ac813-b5f5-4314-834c-5c8e6c901b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the TSQ data (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS2356'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "db0217b1-d7f9-491d-9284-f6f0f7edaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftp.cwd('/pub/databases/metabolights/studies/public/' + study_id)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "640a543f-aadf-4eda-99ca-62e766f6c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FILES',\n",
       " 'HASHES',\n",
       " 'METADATA_REVISIONS',\n",
       " 'a_MTBLS2356_LC-MS_negative__metabolite_profiling.txt',\n",
       " 'a_MTBLS2356_LC-MS_positive__metabolite_profiling.txt',\n",
       " 'i_Investigation.txt',\n",
       " 'm_MTBLS2356_LC-MS_negative__metabolite_profiling_v2_maf.tsv',\n",
       " 'm_MTBLS2356_LC-MS_positive__metabolite_profiling_v2_maf.tsv',\n",
       " 's_MTBLS2356.txt']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b281188c-0bbc-408f-8b88-a3be2e167abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata information so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "\n",
    "# metadataFiles\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "    \n",
    "#ftp.quit()  #can close this down now as I have the files I need (careful when I get to the loop\n",
    "#print(\"FTP closed\")\n",
    "\n",
    "metadata = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8aeae2d7-ffdf-4b52-9fd6-9fd8c29dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull Source Name as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata['Source Name']\n",
    "depth = metadata['Factor Value[Depth]']\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d29c8922-5b7b-4017-bcfb-9559e461a282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2016-07-09T18:04:00\n",
       "1    2016-07-09T18:04:00\n",
       "2    2016-07-09T18:04:00\n",
       "3    2016-07-10T00:02:00\n",
       "4    2016-07-10T00:02:00\n",
       "dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "date_cmap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "### Need BIOS-SCOPE file for lat/lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "988bd0e6-09e1-4813-92bb-989fcf659da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "fName = 'data/BATS_BS_COMBINED_MASTER_latest.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1a245c8b-a8a4-4582-981b-a41bf8b9bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb95e43-079e-4082-8e1f-b2fb86164636",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "00364018-f720-4c0e-8058-8d4c23772743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c35ed1a2-bc00-44f9-9245-0775f37afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 #make a loop later as can have multiple data files for a single dataset\n",
    "writeFile = 'data/' + 'tempData.tsv'\n",
    "readFile = dataFiles.loc[0,'files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0db77f16-d182-4a71-b444-9f86e3ebc0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "        \n",
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9bbbe2a0-5b3e-4e52-9833-6926771daa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the test, can I read in the resulting file? YES, so this is the datafile\n",
    "tsvFile = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start assembling into CMAP format\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2b16a-8361-49d1-a8e5-6e21e59ae5e3",
   "metadata": {},
   "source": [
    "metadata about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "fName = 'datasetTemplate.xlsx'\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadat about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(1,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f7a5c-4310-4e5d-b815-074e1c2c930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#original plan read in the second sheet of the BIOS-SCOPE discrete data file...but that is not at BCO-DMO so I need another plan.\n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "#df2['var_short_name'] = wbVar['Header']\n",
    "#df2['var_long_name'] = wbVar['Description']\n",
    "#df2[,'var_sensor'] = 'need this'\n",
    "#df2['var_unit'] = wbVar['Unit']\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8440118-4769-4e8a-885b-44de2f324d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a9eb734-e142-4c3d-b258-a354df9d8b9a",
   "metadata": {},
   "source": [
    "metadata about the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568a481-d5f6-4ade-889b-e2303afdcba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86bd45-f3f4-4069-b6a0-fd9c21e71b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update this ... can use information from MetaboLights (later)\n",
    "\n",
    "# finally gather up the dataset_meta_data\n",
    "# assemble the details here, might setup in a separate text file later\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "    'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': ['2025-06-25'],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': ['This dataset includes analyses from Niskin bottle samples collected on R/V Atlantic Explorer cruises as part of the BIOS-SCOPE campaign in the time period from 2016 until 2025. Included are CTD data, and survey biogeochemical samples including inorganic nutrients, particulate organic carbon and nitrogen, dissolved organic carbon, dissolved organic nitrogen, total dissolved amino acids, bacterial abundance and production.'],\n",
    "    'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "    'climatology': [0]\n",
    "    })\n",
    "\n",
    "#get the list of cruise names from the bcodmo data file\n",
    "t = pd.DataFrame(bcodmo['Cruise_ID'].unique())\n",
    "t.columns = ['cruise_names']\n",
    "df3 = pd.concat([df3,t],axis=1,ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/forCMAPfromMetabolights.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
