{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 27 June 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station inforamtion to MetaboLights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d55944-de4f-43f0-8cd7-d561a28d929c",
   "metadata": {},
   "source": [
    "# This really needs organizational help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7956b4c3-9847-42dc-97fe-889d7e35f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to organize this a bit better as I have connections to FTP spreadthroughout this...will be easier if \n",
    "#I pull everything I need at once, and then close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bb1fbb45-aad7-45aa-9535-7fde8cac1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ftplib import FTP\n",
    "import re\n",
    "#from openpyxl import Workbook\n",
    "#pd.options.mode.copy_on_write = True #will be the default, may as well set it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "db13a103-9d08-4f8f-b39d-376694a712f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will go here: C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\data_pipeline\\CMAP\\data\n"
     ]
    }
   ],
   "source": [
    "#make the data folder if it is not already there (it is in .gitignore, so it will not end up at GitHub)\n",
    "folder = \"data\"\n",
    "os.chdir(\".\")\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data will go here: %s\" % (os.getcwd()) + '\\\\' + folder)\n",
    "else:\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d45ac813-b5f5-4314-834c-5c8e6c901b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the TSQ data (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS2356'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "db0217b1-d7f9-491d-9284-f6f0f7edaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftp.cwd('/pub/databases/metabolights/studies/public/' + study_id)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "640a543f-aadf-4eda-99ca-62e766f6c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FILES',\n",
       " 'HASHES',\n",
       " 'METADATA_REVISIONS',\n",
       " 'a_MTBLS2356_LC-MS_negative__metabolite_profiling.txt',\n",
       " 'a_MTBLS2356_LC-MS_positive__metabolite_profiling.txt',\n",
       " 'i_Investigation.txt',\n",
       " 'm_MTBLS2356_LC-MS_negative__metabolite_profiling_v2_maf.tsv',\n",
       " 'm_MTBLS2356_LC-MS_positive__metabolite_profiling_v2_maf.tsv',\n",
       " 's_MTBLS2356.txt']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b281188c-0bbc-408f-8b88-a3be2e167abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata information so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "\n",
    "# metadataFiles\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "    \n",
    "#ftp.quit()  #can close this down now as I have the files I need (careful when I get to the loop\n",
    "#print(\"FTP closed\")\n",
    "\n",
    "metadata = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8aeae2d7-ffdf-4b52-9fd6-9fd8c29dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull Source Name as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata['Source Name']\n",
    "depth = metadata['Factor Value[Depth]']\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d29c8922-5b7b-4017-bcfb-9559e461a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "# date_cmap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "### Need BIOS-SCOPE file for lat/lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "988bd0e6-09e1-4813-92bb-989fcf659da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "fName = 'data/BATS_BS_COMBINED_MASTER_latest.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a245c8b-a8a4-4582-981b-a41bf8b9bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e503b12-703e-43ba-b881-846829c79de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also need list of cruises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d2acc-f945-4883-8e30-64b91007d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb95e43-079e-4082-8e1f-b2fb86164636",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00364018-f720-4c0e-8058-8d4c23772743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c35ed1a2-bc00-44f9-9245-0775f37afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 #make a loop later as can have multiple data files for a single dataset\n",
    "writeDataFile = 'data/' + 'tempData.tsv'\n",
    "readDataFile = dataFiles.loc[0,'files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "98084ba8-afd3-4a7a-922c-8648a99a1ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m_MTBLS2356_LC-MS_negative__metabolite_profiling_v2_maf.tsv'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFiles.loc[0,'files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3b8d2f33-fc82-452f-9aa1-bc2998464912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "with open(writeDataFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readDataFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f9d0d589-9b8a-449f-bd28-b9a5b4791a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#details about the experiment are easy since the filename is generic\n",
    "writeFile = 'data/' + 'i_Investigation.txt'\n",
    "readFile = 'i_Investigation.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "\n",
    "del writeFile, readFile\n",
    "##will use that file later, for now move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b30866ef-4e5a-4835-80b8-10a262a032b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9bbbe2a0-5b3e-4e52-9833-6926771daa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on the data file first\n",
    "#now the test, can I read in the resulting file? YES, so this is the datafile\n",
    "tsvFile = pd.read_table(writeDataFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "227c8a99-440c-4046-a73d-57ca39e29bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['database_identifier', 'chemical_formula', 'smiles', 'inchi',\n",
       "       'metabolite_identification', 'mass_to_charge', 'fragmentation',\n",
       "       'modifications', 'charge', 'retention_time',\n",
       "       ...\n",
       "       's1035700708', 's1035700712', 's1035700704', 's1035700718',\n",
       "       's1035802401', 's1035802424', 's1035802408', 's1035802412',\n",
       "       's1035802404', 's1035802418'],\n",
       "      dtype='object', length=393)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information, will also use this for the sheet with metadata about the variables\n",
    "nVariables = len(dataOnly.columns) #need this for the sheet for the metadata on the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start assembling into CMAP format\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2b16a-8361-49d1-a8e5-6e21e59ae5e3",
   "metadata": {},
   "source": [
    "metadata about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "fName = 'datasetTemplate.xlsx'\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(0,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "68e818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to details about the metabolites\n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "df2['var_short_name'] = dataOnly.columns\n",
    "df2.loc[:,'var_long_name'] = tsvFile.loc[:,'metabolite_identification']\n",
    "df2.loc[:,'var_sensor'] = 'Triple quadrupole mass spectrometer (TSQ Vantage, Thermo Scientific)'\n",
    "df2.loc[:,'var_unit'] = 'pM' #this is in the protocols, but I also have some inside information here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "df2.loc[:,('var_discipline')] = 'chemistry'\n",
    "df2.loc[:,('var_visualize')] = 1 #yes/no, all metabolites can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e8440118-4769-4e8a-885b-44de2f324d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'var_keywords' will be the hardest as the metabolites have many, many keywords. \n",
    "#I want to talk to the people at CMAP about best options to handle that (start a list of topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9eb734-e142-4c3d-b258-a354df9d8b9a",
   "metadata": {},
   "source": [
    "metadata about the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c35f8d20-3976-49e3-8923-8757307e72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about the project is in the i_Investigation file, read in the file and pull details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6e48d69a-0148-4d5f-ba15-7771ac2f89b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Organic carbon in seawater plays a significant role in the global carbon cycle. The concentration and composition of dissolved organic carbon, operationally defined in this project as organic carbon that passes through a 0.2 Âµm filter, reflect the actions of the biological community and chemical reactions that occur in seawater. Here, we repeatedly sampled the oligotrophic northwest Sargasso Sea in the vicinity of the Bermuda Atlantic Time-Series Study site (BATS) to quantitatively follow select known compounds within the pool of dissolved organic matter in the upper 1000 meters of the water column over a four-year period. Metabolite concentrations revealed patterns with depth and time with most metabolites showing surface enrichment and lower concentrations with depth. Select metabolites had a pattern of increased and decreased concentrations throughout the year, which was observed in each of the years sampled. Vitamins, including pantothenic acid, biotin, and riboflavin, presented annual increases in the winter period when mixed layer depths are deepest. Light-sensitive riboflavin also showed significant decreases during daylight hours under diel sampling. The metabolites examined in this study are all components of central carbon metabolism. By examining these metabolites at finer resolution and in a relatively long time series, we have clues on microbial actions in marine systems, data which are fundamental&nbsp;to understanding the chemical response of marine systems to future changes in climate.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open up the txt file with the experiment data\n",
    "file_path = 'data/i_Investigation.txt'  \n",
    "with open(file_path, 'r') as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "# pattern = r'NAME = (.*)' # Captures anything after \"NAME = \"\n",
    "pattern = r'Study Description(.*)' # Captures anything after \"NAME = \"\n",
    "#pattern = r'\\d+'  # Matches one or more digits\n",
    "extracted_data = re.findall(pattern, text_content) #this is a list\n",
    "extracted_data = ' '.join(extracted_data) # ? really, this seems odd, but works.\n",
    "\n",
    "#tidy up the string\n",
    "original_string = extracted_data\n",
    "chars_to_remove = ['<p>', '</p>','\\t']\n",
    "\n",
    "# # Using iteration and replace()\n",
    "# modified_string_replace = original_string\n",
    "# for char in chars_to_remove:\n",
    "#     modified_string_replace = modified_string_replace.replace(char, \"\")\n",
    "# print(f\"String after replace: {modified_string_replace}\")\n",
    "\n",
    "# Using regular expressions (for more complex patterns or multiple occurrences)\n",
    "pattern = \"|\".join(map(re.escape, chars_to_remove)) # Escapes special characters for regex\n",
    "project_description = re.sub(pattern, \"\", original_string)\n",
    "project_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "77721875-58ef-4cd1-97b0-a4a992877e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the dataset_meta_data into df3\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "    'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': ['2025-06-25'],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': [project_description],\n",
    "    'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "    'climatology': [0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4e86bd45-f3f4-4069-b6a0-fd9c21e71b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assemble the details here, might setup in a separate text file later\n",
    "# df3 = pd.DataFrame({\n",
    "#     'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "#     'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "#     'dataset_version': ['1.0'],\n",
    "#     'dataset_release_date': ['2025-06-25'],\n",
    "#     'dataset_make': ['observation'],\n",
    "#     'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "#     'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "#     'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "#     'dataset_history': [''],\n",
    "#     'dataset_description': ['This dataset includes analyses from Niskin bottle samples collected on R/V Atlantic Explorer cruises as part of the BIOS-SCOPE campaign in the time period from 2016 until 2025. Included are CTD data, and survey biogeochemical samples including inorganic nutrients, particulate organic carbon and nitrogen, dissolved organic carbon, dissolved organic nitrogen, total dissolved amino acids, bacterial abundance and production.'],\n",
    "#     'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "#     'climatology': [0]\n",
    "#     })\n",
    "\n",
    "# #get the list of cruise names from the bcodmo data file\n",
    "# t = pd.DataFrame(bcodmo['Cruise_ID'].unique())\n",
    "# t.columns = ['cruise_names']\n",
    "# df3 = pd.concat([df3,t],axis=1,ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/forCMAPfromMetabolights.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233c6-73ef-4e1a-87e8-e9b627977a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a189960-4354-4b44-85b7-f17a2eaa18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise UserWarning('Stopping and leave code below for historical reasons, code will not run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca46aa-613d-4b3a-a480-11fd7a4bd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I can get isatools to install, this will be an easier way (I think/hope) to access data at BCO-DMO. There is also an R equivalent at metabolighteR that provides access to MetaboLights REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a526e-326d-408d-8e41-ac117cfcfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install isatools #fails with this error:AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
    "pip install git+https://github.com/ISA-tools/isa-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f991bc-f47e-4cf6-8670-f856c0764fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71df46-62b2-44a2-8330-b2b58c2b02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isatools.net import mtbls as MTBLS\n",
    "tmp_dir = MTBLS.get('MTBLS2356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdef19-fe01-412b-8b46-15ef0fdaf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
