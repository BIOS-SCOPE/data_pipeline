{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 9 July 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station information to MetaboLights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bb1fbb45-aad7-45aa-9535-7fde8cac1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from ftplib import FTP\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "#import libchebipy #pip install libChEBIpy\n",
    "from libchebipy._chebi_entity import ChebiEntity #this will get metabolite synonyms\n",
    "\n",
    "# import pdb\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "db13a103-9d08-4f8f-b39d-376694a712f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will go here (but should not be synced to GitHub): C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\data_pipeline\\CMAP\\data\n"
     ]
    }
   ],
   "source": [
    "#make the data folder if it is not already there (it is in .gitignore, so it will not end up at GitHub)\n",
    "folder = \"data\"\n",
    "os.chdir(\".\")\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data will go here (but should not be synced to GitHub): %s\" % (os.getcwd()) + '\\\\' + folder)\n",
    "else:\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff0279-52e0-4e2f-9fd1-81049aa61cdd",
   "metadata": {},
   "source": [
    "### Get MetaboLights files via FTP access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a7d19-9347-4dcd-88c3-e10fda32ca5c",
   "metadata": {},
   "source": [
    "The following cells read a file, write to disk, and then read the result into Python. I am not sure how to skip the middle/write step, but it seems like setting this up should work (but it doesn't)\\\n",
    "    # Create an in-memory binary stream\\\n",
    "    in_memory_file = io.BytesIO()\n",
    "\n",
    "Note: the frictionless website talks about FTP access to get data, but I cannot get it to work (by guessing, seems undocumented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b3829600-5ab2-4c79-8bd8-bddd96b13cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with one dataset at MetaboLights, TSQ data described in Longnecker et al. 2024 (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS2356'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2e7d3689-ac17-40c8-8aad-b69625a2cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftpDataAddress = '/pub/databases/metabolights/studies/public/' + study_id\n",
    "ftp.cwd(ftpDataAddress)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable\n",
    "#fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8f1e7411-1324-4eca-bbe0-f396839be402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata about the samples so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "# metadataFiles: put them here \n",
    "# Is there a way to download an FTP file and not write it disk?\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "\n",
    "# now read in the result\n",
    "metadata_aboutSamples = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0259b25e-a334-437b-a68d-1d74dead48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list\n",
    "tsvFile = pd.DataFrame(); #this will be the data file\n",
    "\n",
    "# readDataFile = dataFiles.loc[0,'files']\n",
    "#have to do some concatenating here bc positive and negative ion mode data\n",
    "for idx in range(len(dataFiles)):\n",
    "    readDataFile = dataFiles.loc[idx,'files']\n",
    "    writeDataFile = 'data/' + 'tempData.tsv'\n",
    "          \n",
    "    with open(writeDataFile,'wb') as fp:\n",
    "        #try-except to make sure the FTP closes\n",
    "        try:\n",
    "            retr_command = f\"RETR {readDataFile}\"\n",
    "            #pdb.set_trace()\n",
    "            ftp.retrbinary(retr_command, fp.write)\n",
    "        except Exception as e: \n",
    "            print(f\"Error during quit: {e}\")\n",
    "    \n",
    "    #read in the temporary file and add to tsvFile file\n",
    "    tsvFile = pd.concat([tsvFile,pd.read_table(writeDataFile,delimiter = '\\t')],ignore_index=True) #append is no longer valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35ec9562-a2d9-4647-845f-9ba58a378428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, details about the experiment are easy because the filename is generic\n",
    "#information about the project is in the i_Investigation file\n",
    "writeFile = 'data/' + 'i_Investigation.txt'\n",
    "readFile = 'i_Investigation.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "\n",
    "# del writeFile, readFile\n",
    "\n",
    "#open up the txt file with the experiment data\n",
    "writeFile = 'data/i_Investigation.txt'  \n",
    "with open(writeFile, 'r') as f:\n",
    "    metadata_aboutExperiment = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b59aba8d-69c4-437a-a6eb-bd87abfc7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this might not be necessary, but leave here in case I go back to it.\n",
    "# files = {}\n",
    "# files['data'] = tsvFile #this also holds the metadata_variables\n",
    "# files['metadata_project'] = metadata_aboutExperiment\n",
    "# files['metadata_samples'] = metadata_aboutSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "46331992-041c-4e3a-b5e4-f7b447a5bad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "522aeca3-1a61-4685-bcfe-5726db85be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull what I can from the sample information at MetaboLights\n",
    "#pull 'Source Name' as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata_aboutSamples['Source Name']\n",
    "depth = metadata_aboutSamples['Factor Value[Depth]']\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata_aboutSamples[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']\n",
    "\n",
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "#### Need BIOS-SCOPE discrete file for lat/lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6494987b-2dd1-406c-8481-24226176484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "#this is slow, so leave as its own cell\n",
    "fName = 'data/BATS_BS_COMBINED_MASTER_latest.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6a9bb7f3-8356-4d6f-8450-b8946c4ac59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then setup the ability to use the information in BSdata by mathing sample names:\n",
    "\n",
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order; only keep rows with data found in MetaboLights\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')\n",
    "\n",
    "#get the list of cruise names and find the unique ones - this will go on the sheet with metadata about the project\n",
    "unCruises = pd.DataFrame(merged_df.loc[:,'Cruise_ID'].unique())\n",
    "unCruises.columns = ['cruise_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2610669-38c2-4bb5-9474-e220179dfbfc",
   "metadata": {},
   "source": [
    "### Operate on the files I collected via FTP\n",
    "Remember data are in the tsvFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "227c8a99-440c-4046-a73d-57ca39e29bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['database_identifier', 'chemical_formula', 'smiles', 'inchi',\n",
       "       'metabolite_identification', 'mass_to_charge', 'fragmentation',\n",
       "       'modifications', 'charge', 'retention_time',\n",
       "       ...\n",
       "       's1035700708', 's1035700712', 's1035700704', 's1035700718',\n",
       "       's1035802401', 's1035802424', 's1035802408', 's1035802412',\n",
       "       's1035802404', 's1035802418'],\n",
       "      dtype='object', length=393)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information, will also use this for the sheet with metadata about the variables\n",
    "nVariables = len(dataOnly.columns) #need this for the sheet for the metadata on the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assemble the data into the CMAP format\n",
    "'''\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "'''\n",
    "fName = 'datasetTemplate.xlsx' #downloaded from CMAP website\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(0,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ecb35bc3-5bb8-4888-bd4c-bc3bd6968ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Define a funtion to use the the chebi tool to find synonyms. Krista Longnecker, 12 July 2025\n",
    "    input: oneName is a string with the CHEBI number (just the number, no prefix)\n",
    "'''\n",
    "def getSynonym(oneName):\n",
    "    possibles = ChebiEntity(oneName).get_names()\n",
    "    justNames = []\n",
    "    for idx in range(len(possibles)):\n",
    "        justNames.append(possibles[idx].get_name())\n",
    "    \n",
    "    return justNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "68e818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need details (metadata) about the metabolites \n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "df2['var_short_name'] = dataOnly.columns\n",
    "df2.loc[:,'var_long_name'] = tsvFile.loc[:,'metabolite_identification']\n",
    "df2.loc[:,'var_sensor'] = 'Triple quadrupole mass spectrometer (TSQ Vantage, Thermo Scientific)'\n",
    "df2.loc[:,'var_unit'] = 'pM' #this is in the protocols, but I also have some inside information here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "df2.loc[:,('var_discipline')] = 'chemistry'\n",
    "df2.loc[:,('visualize')] = 1 #yes/no, all metabolites can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b751b6a3-63e0-425b-ac85-194e6fc36ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use unCruises to get the list of cruises, put that at the beginning, then add text that will work for the whole project\n",
    "misc_keywords = ','.join(unCruises['cruise_names']) + ', Bermuda Institute of Ocean Sciences, bio, biogeo, biogeochemistry, biology, BIOS, Bottle, cruise, Discrete, in situ, insitu, in-situ, North Atlantic Ocean, observation, '\n",
    "#then need to add the string with the names for each metabolite...do that in the next loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f9c5a946-cca7-453e-9ebe-284d7f87f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through one metabolite at a time and make var_keywords\n",
    "#'var_keywords' will be the hardest as the metabolites have many, many keywords. \n",
    "for idx,item in df2.iterrows():\n",
    "    justNames = ', '.join(getSynonym(df2.loc[idx,'var_short_name'].strip('CHEBI:')))\n",
    "    df2.loc[idx,'var_keywords'] = misc_keywords + justNames\n",
    "    #print(df2.loc[idx,'var_short_name'])\n",
    "    del justNames    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8d026a27-3531-4447-8546-3d5fbf007406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function to pull details from MetaboLights i file\n",
    "#this goes through one at a time...could alter to get all in one function (get stuff, and clean stuff as two steps)\n",
    "def getMetaboLights(pattern,text):\n",
    "    #pattern = r'Study Identifier(.*)' # Captures anything after \"Study Identifier = \"\n",
    "    extracted_data = re.findall(pattern, text) #this is a list\n",
    "    extracted_data = ' '.join(extracted_data) # ? really, this seems odd, but works.\n",
    "    \n",
    "    #tidy up the string\n",
    "    original_string = extracted_data\n",
    "    chars_to_remove = ['<p>', '</p>','\\t']\n",
    "    \n",
    "    # Using regular expressions (for more complex patterns or multiple occurrences)\n",
    "    pattern = \"|\".join(map(re.escape, chars_to_remove)) # Escapes special characters for regex\n",
    "    oneVar = re.sub(pattern, \"\", original_string)\n",
    "    return oneVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6e48d69a-0148-4d5f-ba15-7771ac2f89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gather the details I need from the metadata_aboutExperiment\n",
    "'''\n",
    "project_description = getMetaboLights(r'Study Description(.*)',metadata_aboutExperiment)\n",
    "dataset_short_name = getMetaboLights(r'Study Identifier(.*)',metadata_aboutExperiment)\n",
    "dataset_long_name = getMetaboLights(r'Study Title(.*)',metadata_aboutExperiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "77721875-58ef-4cd1-97b0-a4a992877e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the dataset_meta_data into df3\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': [dataset_short_name],\n",
    "    'dataset_long_name': [dataset_long_name],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': ['2025-06-25'],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Elizabeth Kujawinski, Woods Hole Oceanographic Institution'],\n",
    "    'dataset_distributor': ['Elizabeth Kujawinski, Woods Hole Oceanographic Institution'],\n",
    "    'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': [project_description],\n",
    "    'dataset_references': ['https://www.ebi.ac.uk/metabolights/editor/' + study_id],\n",
    "    'climatology': [0]\n",
    "    })\n",
    "\n",
    "\n",
    "#insert the list of unique cruises from the data pulled from the BIOS-SCOPE discrete file\n",
    "df3 = pd.concat([df3,unCruises],axis=1,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/forCMAPfromMetabolights.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf5fd0-ae4c-4577-ae83-88be8930465f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b3784-ff42-4341-be12-ee3e3d4e10c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c2e7d-def5-4434-9348-f45e4d24e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2edcd-e999-472c-9086-3809606278e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233c6-73ef-4e1a-87e8-e9b627977a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4a189960-4354-4b44-85b7-f17a2eaa18e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UserWarning",
     "evalue": "Stopping and leave code below for historical reasons, code will not run",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserWarning\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUserWarning\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopping and leave code below for historical reasons, code will not run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mUserWarning\u001b[0m: Stopping and leave code below for historical reasons, code will not run"
     ]
    }
   ],
   "source": [
    "raise UserWarning('Stopping and leave code below for historical reasons, code will not run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca46aa-613d-4b3a-a480-11fd7a4bd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I can get isatools to install, this will be an easier way (I think/hope) to access data at BCO-DMO. There is also an R equivalent at metabolighteR that provides access to MetaboLights REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a526e-326d-408d-8e41-ac117cfcfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install isatools #fails with this error:AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
    "pip install git+https://github.com/ISA-tools/isa-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f991bc-f47e-4cf6-8670-f856c0764fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71df46-62b2-44a2-8330-b2b58c2b02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isatools.net import mtbls as MTBLS\n",
    "tmp_dir = MTBLS.get('MTBLS2356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e06cf-f8a9-4450-af95-2622b90a0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a Frictionless Data Package \"\"\"\n",
    "\n",
    "biosscope = Package(name='biosscope-bcodmo-datasets', profile='data-package')\n",
    "biosscope.title = 'Bermuda Institute of Ocean Sciences Simons Collaboration on Ocean Processes and Ecology'\n",
    "biosscope.description = 'BIOSSCOPE datasets from BCO-DMO'\n",
    "biosscope.created = rfc3339_datetime_str()\n",
    "biosscope.sources = []\n",
    "\n",
    "for index, dataset in metadata.iterrows():\n",
    "\n",
    "  # Save the dataset as a 'source' in the Package\n",
    "  source = {\n",
    "    'path': dataset['dataset'],\n",
    "    'title': dataset['title'],\n",
    "    'doi': dataset['doi']\n",
    "  }\n",
    "  biosscope.sources.append(source)\n",
    "\n",
    "\n",
    "  # Get the BCO-DMO parameters\n",
    "  parameters = get_sparql_dataframe(SPARQL_ENDPOINT, DATASET_PARAMS_QUERY.replace('{dataset_uri}', dataset['dataset']))\n",
    "  schema = []\n",
    "  for param_index, parameter in parameters.iterrows():\n",
    "    param = {}\n",
    "    param['bcodmo:name'] = parameter['supplied_name']\n",
    "    if parameter['supplied_definition'] is not None:\n",
    "      param['bcodmo:description'] = parameter['supplied_definition']\n",
    "    if parameter['datatype'] is not None:\n",
    "      param['bcodmo:datatype'] = parameter['datatype']\n",
    "    if parameter['units'] is not None:\n",
    "      param['bcodmo:units'] = parameter['units']\n",
    "    if parameter['format'] is not None:\n",
    "      param['bcodmo:valueFormat'] = parameter['format']\n",
    "    schema.append(param)\n",
    "\n",
    "  # Get the 'data' files for a Dataset (skip any supplemental documentation)\n",
    "  files = get_sparql_dataframe(SPARQL_ENDPOINT, DATASET_FILES_QUERY.replace('{dataset_uri}', dataset['dataset']))\n",
    "  for file_index, file in files.iterrows():\n",
    "\n",
    "    # Use Frictionless to describe the file\n",
    "    datafile = describe(file['url'])\n",
    "\n",
    "    # Get Table stats\n",
    "    if datafile.type == 'table':\n",
    "      datafile.infer(stats=True)\n",
    "\n",
    "    # Specify which dataset this file belongs to\n",
    "    datafile.sources = [source]\n",
    "\n",
    "    # If the file is marked as the primary file for the dataset, attach the parameters to the file\n",
    "    if schema is not None and file['is_primary_data_file'] == 'true':\n",
    "      datafile.custom['bcodmo:parameters'] = schema\n",
    "\n",
    "    # Add the file to the package\n",
    "    biosscope.add_resource(datafile)\n",
    "\n",
    "# Save the package\n",
    "print(biosscope.to_json())\n",
    "biosscope.to_json('datapackage.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdef19-fe01-412b-8b46-15ef0fdaf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
