{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 8 July 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station inforamtion to MetaboLights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d55944-de4f-43f0-8cd7-d561a28d929c",
   "metadata": {},
   "source": [
    "# This really needs organizational help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98361eac-02b1-46cc-8224-7e0fbee589a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7956b4c3-9847-42dc-97fe-889d7e35f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to organize this a bit better as I have connections to FTP spreadthroughout this...will be easier if \n",
    "#I pull everything I need at once, and then close the connection\n",
    "#working on the organization now - I am going to assemble the MetaboLights data into a frictionless data package. \n",
    "#This way I can reuse code that I already wrote for the BCO-DMO data.\n",
    "##get all the pieces first and then do things with the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb1fbb45-aad7-45aa-9535-7fde8cac1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from ftplib import FTP\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import json\n",
    "from frictionless import describe, Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05787781-ce5e-4d8b-a701-d9ff1a06caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" HELPER FUNCTIONS \"\"\"\n",
    "def rfc3339_datetime_str():\n",
    "    \"\"\"\n",
    "    Construct an RFC3339-compliant datetime\n",
    "    \"\"\"\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db13a103-9d08-4f8f-b39d-376694a712f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will go here (but should not be synced to GitHub): C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\data_pipeline\\CMAP\\data\n"
     ]
    }
   ],
   "source": [
    "#make the data folder if it is not already there (it is in .gitignore, so it will not end up at GitHub)\n",
    "folder = \"data\"\n",
    "os.chdir(\".\")\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data will go here (but should not be synced to GitHub): %s\" % (os.getcwd()) + '\\\\' + folder)\n",
    "else:\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a7d19-9347-4dcd-88c3-e10fda32ca5c",
   "metadata": {},
   "source": [
    "First - set up the FTP access to MetaboLights and get the files I want\n",
    "The following cells read a file, write to disk, and then read the result into Python. I am not sure\n",
    "how to skip the middle/write step, but it seems like setting this up should work (but it doesn't)\\\n",
    "    # Create an in-memory binary stream\\\n",
    "    in_memory_file = io.BytesIO()\n",
    "\n",
    "Note: the frictionless website talks about FTP access to get data, but I cannot get it to work (by guessing, seems undocumented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b3829600-5ab2-4c79-8bd8-bddd96b13cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with one dataset at MetaboLights, TSQ data described in Longnecker et al. 2024 (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS2356'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2e7d3689-ac17-40c8-8aad-b69625a2cefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FILES',\n",
       " 'HASHES',\n",
       " 'METADATA_REVISIONS',\n",
       " 'a_MTBLS2356_LC-MS_negative__metabolite_profiling.txt',\n",
       " 'a_MTBLS2356_LC-MS_positive__metabolite_profiling.txt',\n",
       " 'i_Investigation.txt',\n",
       " 'm_MTBLS2356_LC-MS_negative__metabolite_profiling_v2_maf.tsv',\n",
       " 'm_MTBLS2356_LC-MS_positive__metabolite_profiling_v2_maf.tsv',\n",
       " 's_MTBLS2356.txt']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftpDataAddress = '/pub/databases/metabolights/studies/public/' + study_id\n",
    "ftp.cwd(ftpDataAddress)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "158f707b-97c6-44cd-a577-649f34de40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'ftp://ftp.ebi.ac.uk' + ftpDataAddress + '/' + 's_MTBLS2356.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5dfaedce-65e5-47b8-a52d-d55c4a9d8dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ftp://ftp.ebi.ac.uk/pub/databases/metabolights/studies/public/MTBLS2356/s_MTBLS2356.txt'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6892eef2-2d5f-4ef3-a5fa-803f72f9c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from frictionless import Resource\n",
    "\n",
    "path=test\n",
    "resource = Resource(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ce78690b-a9cf-49e6-b3ec-84415a8946d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 's_mtbls2356',\n",
       " 'type': 'text',\n",
       " 'path': 'ftp://ftp.ebi.ac.uk/pub/databases/metabolights/studies/public/MTBLS2356/s_MTBLS2356.txt',\n",
       " 'scheme': 'ftp',\n",
       " 'format': 'txt',\n",
       " 'mediatype': 'text/txt'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e4ae1417-06e5-4e45-ae98-afdea1f87c99",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextResource' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m datafile \u001b[38;5;241m=\u001b[39m describe(resource[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextResource' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "datafile = describe(resource['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f1e7411-1324-4eca-bbe0-f396839be402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata about the samples so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "# metadataFiles: put them here \n",
    "# Is there a way to download an FTP file and not write it disk?\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "\n",
    "# now read in the result\n",
    "metadata_aboutSamples = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0259b25e-a334-437b-a68d-1d74dead48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list\n",
    "readDataFile = dataFiles.loc[0,'files']\n",
    "\n",
    "idx = 0 #make a loop later as can have multiple data files for a single dataset\n",
    "writeDataFile = 'data/' + 'tempData.tsv'\n",
    "\n",
    "#dataFiles.loc[0,'files'] #use this to see the file\n",
    "\n",
    "with open(writeDataFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readDataFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "\n",
    "\n",
    "#now read in the resulting file? \n",
    "#see question above, can I just read this in and NOT write to disk?\n",
    "tsvFile = pd.read_table(writeDataFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35ec9562-a2d9-4647-845f-9ba58a378428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, details about the experiment are easy because the filename is generic\n",
    "writeFile = 'data/' + 'i_Investigation.txt'\n",
    "readFile = 'i_Investigation.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "\n",
    "# del writeFile, readFile\n",
    "\n",
    "#open up the txt file with the experiment data\n",
    "writeFile = 'data/i_Investigation.txt'  \n",
    "with open(writeFile, 'r') as f:\n",
    "    metadata_aboutExperiment = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "46331992-041c-4e3a-b5e4-f7b447a5bad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2610669-38c2-4bb5-9474-e220179dfbfc",
   "metadata": {},
   "source": [
    "Now do stuff with the files I just collected: make a frictionless package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff055c07-b224-4020-b3ee-656b964ea90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the frictionless package, just modify the syntax I have\n",
    "\"\"\" Create a Frictionless Data Package \"\"\"\n",
    "\n",
    "metabolights = Package(name='metabolights-datasets', profile='data-package')\n",
    "metabolights.title = 'Bermuda Institute of Ocean Sciences Simons Collaboration on Ocean Processes and Ecology'\n",
    "metabolights.description = 'Kujawinski laboratory datasets from MetaboLights' #need to update as needed\n",
    "metabolights.created = rfc3339_datetime_str()\n",
    "metabolights.sources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ffe1b96e-b5d2-4082-a0c0-95274fec522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'metabolights-datasets',\n",
       " 'title': 'Bermuda Institute of Ocean Sciences Simons Collaboration on Ocean '\n",
       "          'Processes and Ecology',\n",
       " 'description': 'Kujawinski laboratory datasets from MetaboLights',\n",
       " 'profile': 'data-package',\n",
       " 'sources': [],\n",
       " 'created': '2025-07-09T12:53:33.303756Z'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metabolights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "79ab4aa6-85bc-4240-aa14-7c051d03d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to follow the BCO-DMO format (not clear that I am actually doing that...but try)\n",
    "# Save the dataset as a 'source' in the frictionless package\n",
    "source = {\n",
    "'path': ftpDataAddress,\n",
    "'title': study_id,\n",
    "#'doi': dataset['doi']\n",
    "}\n",
    "metabolights.sources.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "020dc42d-f8d4-411f-9103-fa135c1ce04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'metabolights-datasets',\n",
       " 'title': 'Bermuda Institute of Ocean Sciences Simons Collaboration on Ocean '\n",
       "          'Processes and Ecology',\n",
       " 'description': 'Kujawinski laboratory datasets from MetaboLights',\n",
       " 'profile': 'data-package',\n",
       " 'sources': [{'path': '/pub/databases/metabolights/studies/public/MTBLS2356',\n",
       "              'title': 'MTBLS2356'}],\n",
       " 'created': '2025-07-09T12:53:33.303756Z'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metabolights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b0ca8-dec7-46da-b979-72a754374505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b59aba8d-69c4-437a-a6eb-bd87abfc7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "files['data'] = tsvFile #this also holds the metadata_variables\n",
    "files['metadata_project'] = metadata_aboutExperiment\n",
    "files['metadata_samples'] = metadata_aboutSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "70bb79dd-34d3-4018-82b4-3b075046a8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'package'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metabolights\u001b[38;5;241m.\u001b[39madd_resource(files)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\package\\package.py:206\u001b[0m, in \u001b[0;36mPackage.add_resource\u001b[1;34m(self, resource)\u001b[0m\n\u001b[0;32m    204\u001b[0m     resource \u001b[38;5;241m=\u001b[39m Resource\u001b[38;5;241m.\u001b[39mfrom_descriptor(resource, basepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasepath)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mappend(resource)\n\u001b[1;32m--> 206\u001b[0m resource\u001b[38;5;241m.\u001b[39mpackage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resource\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'package'"
     ]
    }
   ],
   "source": [
    "metabolights.add_resource(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3fb61-fac1-4c18-946c-79a94f4d27b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bee6b1cf-01d1-4b21-ae3a-19a376119b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "metabolights.sources.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f2b43899-fa45-49e4-96b9-10c3430bb981",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_descriptor_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[0;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[0;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[0;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> 708\u001b[0m printer\u001b[38;5;241m.\u001b[39mpretty(obj)\n\u001b[0;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\lib\\pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[0;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m _repr_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\lib\\pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(obj)\n\u001b[0;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\metadata\\metadata.py:77\u001b[0m, in \u001b[0;36mMetadata.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pprint\u001b[38;5;241m.\u001b[39mpformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_descriptor(), sort_dicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\metadata\\metadata.py:198\u001b[0m, in \u001b[0;36mMetadata.to_descriptor\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_descriptor\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m types\u001b[38;5;241m.\u001b[39mIDescriptor:\n\u001b[1;32m--> 198\u001b[0m     descriptor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_export()\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m    200\u001b[0m         Error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_Error \u001b[38;5;129;01mor\u001b[39;00m platform\u001b[38;5;241m.\u001b[39mfrictionless_errors\u001b[38;5;241m.\u001b[39mMetadataError\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\package\\package.py:743\u001b[0m, in \u001b[0;36mPackage.metadata_export\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata_export\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m     descriptor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmetadata_export()\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;66;03m# TODO: recover after standards v2 release?\u001b[39;00m\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;66;03m# Frictionless\u001b[39;00m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m#  if system.standards == \"v2\":\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m#  descriptor = {\"$frictionless\": \"package/v2\", **descriptor}\u001b[39;00m\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m descriptor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\metadata\\metadata.py:447\u001b[0m, in \u001b[0;36mMetadata.metadata_export\u001b[1;34m(self, exclude)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Class:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 447\u001b[0m         value \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mto_descriptor_source() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto_descriptor_source()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\frictionless\\metadata\\metadata.py:447\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Class:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 447\u001b[0m         value \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mto_descriptor_source() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto_descriptor_source()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_descriptor_source'"
     ]
    }
   ],
   "source": [
    "metabolights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f96893-7771-4ac6-9ce0-0257aa79beba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cede4-0518-4f41-b48c-433f8a6c4987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1e28b-a013-404a-a471-72294f3823eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1455e3-7c7b-4d7b-8fe4-9ad394b7eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the package\n",
    "print(metabolights.to_json())\n",
    "metabolights.to_json('datapackage.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd06a52-afe1-46e8-8da9-cf86e2c27b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281188c-0bbc-408f-8b88-a3be2e167abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeae2d7-ffdf-4b52-9fd6-9fd8c29dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull Source Name as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata_aboutSamples['Source Name']\n",
    "depth = metadata_aboutSamples['Factor Value[Depth]']\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata_aboutSamples[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c8922-5b7b-4017-bcfb-9559e461a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "# date_cmap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "### Need BIOS-SCOPE file for lat/lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988bd0e6-09e1-4813-92bb-989fcf659da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "fName = 'data/BATS_BS_COMBINED_MASTER_latest.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a245c8b-a8a4-4582-981b-a41bf8b9bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e503b12-703e-43ba-b881-846829c79de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also need list of cruises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d2acc-f945-4883-8e30-64b91007d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb95e43-079e-4082-8e1f-b2fb86164636",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea8155-6c59-4c5a-b22c-dbbd266df65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d2f33-fc82-452f-9aa1-bc2998464912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0d589-9b8a-449f-bd28-b9a5b4791a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30866ef-4e5a-4835-80b8-10a262a032b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbe2a0-5b3e-4e52-9833-6926771daa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8a99-440c-4046-a73d-57ca39e29bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information, will also use this for the sheet with metadata about the variables\n",
    "nVariables = len(dataOnly.columns) #need this for the sheet for the metadata on the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start assembling into CMAP format\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2b16a-8361-49d1-a8e5-6e21e59ae5e3",
   "metadata": {},
   "source": [
    "metadata about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "fName = 'datasetTemplate.xlsx'\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(0,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to details about the metabolites\n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "df2['var_short_name'] = dataOnly.columns\n",
    "df2.loc[:,'var_long_name'] = tsvFile.loc[:,'metabolite_identification']\n",
    "df2.loc[:,'var_sensor'] = 'Triple quadrupole mass spectrometer (TSQ Vantage, Thermo Scientific)'\n",
    "df2.loc[:,'var_unit'] = 'pM' #this is in the protocols, but I also have some inside information here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "df2.loc[:,('var_discipline')] = 'chemistry'\n",
    "df2.loc[:,('var_visualize')] = 1 #yes/no, all metabolites can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8440118-4769-4e8a-885b-44de2f324d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'var_keywords' will be the hardest as the metabolites have many, many keywords. \n",
    "#I want to talk to the people at CMAP about best options to handle that (start a list of topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952720f1-17b7-4b87-a008-f72783d37030",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This publication may be useful in getting information about metabolites via SPARQL queries\n",
    "# https://jcheminf.biomedcentral.com/articles/10.1186/s13321-016-0144-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9eb734-e142-4c3d-b258-a354df9d8b9a",
   "metadata": {},
   "source": [
    "metadata about the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f8d20-3976-49e3-8923-8757307e72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about the project is in the i_Investigation file, read in the file and pull details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48d69a-0148-4d5f-ba15-7771ac2f89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pattern = r'NAME = (.*)' # Captures anything after \"NAME = \"\n",
    "pattern = r'Study Description(.*)' # Captures anything after \"NAME = \"\n",
    "#pattern = r'\\d+'  # Matches one or more digits\n",
    "extracted_data = re.findall(pattern, metadata_aboutExperiment) #this is a list\n",
    "extracted_data = ' '.join(extracted_data) # ? really, this seems odd, but works.\n",
    "\n",
    "#tidy up the string\n",
    "original_string = extracted_data\n",
    "chars_to_remove = ['<p>', '</p>','\\t']\n",
    "\n",
    "# # Using iteration and replace()\n",
    "# modified_string_replace = original_string\n",
    "# for char in chars_to_remove:\n",
    "#     modified_string_replace = modified_string_replace.replace(char, \"\")\n",
    "# print(f\"String after replace: {modified_string_replace}\")\n",
    "\n",
    "# Using regular expressions (for more complex patterns or multiple occurrences)\n",
    "pattern = \"|\".join(map(re.escape, chars_to_remove)) # Escapes special characters for regex\n",
    "project_description = re.sub(pattern, \"\", original_string)\n",
    "project_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77721875-58ef-4cd1-97b0-a4a992877e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the dataset_meta_data into df3\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "    'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': ['2025-06-25'],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "    'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': [project_description],\n",
    "    'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "    'climatology': [0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86bd45-f3f4-4069-b6a0-fd9c21e71b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assemble the details here, might setup in a separate text file later\n",
    "# df3 = pd.DataFrame({\n",
    "#     'dataset_short_name': ['BIOSSCOPE_v1'],\n",
    "#     'dataset_long_name': ['BIOS-SCOPE discrete sample data'],\n",
    "#     'dataset_version': ['1.0'],\n",
    "#     'dataset_release_date': ['2025-06-25'],\n",
    "#     'dataset_make': ['observation'],\n",
    "#     'dataset_source': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "#     'dataset_distributor': ['Craig Carlson, Bermuda Institute of Ocean Sciences'],\n",
    "#     'dataset_acknowledgement': ['We thank the BIOS-SCOPE project team and the BATS team for assistance with sample collection, processing, and analysis. The efforts of the captains, crew, and marine technicians of the R/V Atlantic Explorer are a key aspect of the success of this project. This work supported by funding from the Simons Foundation International.'],\n",
    "#     'dataset_history': [''],\n",
    "#     'dataset_description': ['This dataset includes analyses from Niskin bottle samples collected on R/V Atlantic Explorer cruises as part of the BIOS-SCOPE campaign in the time period from 2016 until 2025. Included are CTD data, and survey biogeochemical samples including inorganic nutrients, particulate organic carbon and nitrogen, dissolved organic carbon, dissolved organic nitrogen, total dissolved amino acids, bacterial abundance and production.'],\n",
    "#     'dataset_references': ['Carlson, C. A., Giovannoni, S., Liu, S., Halewood, E. (2025) BIOS-SCOPE survey biogeochemical data as collected on Atlantic Explorer cruises (AE1614, AE1712, AE1819, AE1916) from 2016 through 2019. Biological and Chemical Oceanography Data Management Office (BCO-DMO). (Version 1) Version Date 2021-10-17. doi:10.26008/1912/bco-dmo.861266.1 [25 June 2025]'],\n",
    "#     'climatology': [0]\n",
    "#     })\n",
    "\n",
    "# #get the list of cruise names from the bcodmo data file\n",
    "# t = pd.DataFrame(bcodmo['Cruise_ID'].unique())\n",
    "# t.columns = ['cruise_names']\n",
    "# df3 = pd.concat([df3,t],axis=1,ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/forCMAPfromMetabolights.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf5fd0-ae4c-4577-ae83-88be8930465f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b3784-ff42-4341-be12-ee3e3d4e10c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c2e7d-def5-4434-9348-f45e4d24e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2edcd-e999-472c-9086-3809606278e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233c6-73ef-4e1a-87e8-e9b627977a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a189960-4354-4b44-85b7-f17a2eaa18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise UserWarning('Stopping and leave code below for historical reasons, code will not run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca46aa-613d-4b3a-a480-11fd7a4bd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I can get isatools to install, this will be an easier way (I think/hope) to access data at BCO-DMO. There is also an R equivalent at metabolighteR that provides access to MetaboLights REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a526e-326d-408d-8e41-ac117cfcfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install isatools #fails with this error:AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
    "pip install git+https://github.com/ISA-tools/isa-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f991bc-f47e-4cf6-8670-f856c0764fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71df46-62b2-44a2-8330-b2b58c2b02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isatools.net import mtbls as MTBLS\n",
    "tmp_dir = MTBLS.get('MTBLS2356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e06cf-f8a9-4450-af95-2622b90a0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a Frictionless Data Package \"\"\"\n",
    "\n",
    "biosscope = Package(name='biosscope-bcodmo-datasets', profile='data-package')\n",
    "biosscope.title = 'Bermuda Institute of Ocean Sciences Simons Collaboration on Ocean Processes and Ecology'\n",
    "biosscope.description = 'BIOSSCOPE datasets from BCO-DMO'\n",
    "biosscope.created = rfc3339_datetime_str()\n",
    "biosscope.sources = []\n",
    "\n",
    "for index, dataset in metadata.iterrows():\n",
    "\n",
    "  # Save the dataset as a 'source' in the Package\n",
    "  source = {\n",
    "    'path': dataset['dataset'],\n",
    "    'title': dataset['title'],\n",
    "    'doi': dataset['doi']\n",
    "  }\n",
    "  biosscope.sources.append(source)\n",
    "\n",
    "\n",
    "  # Get the BCO-DMO parameters\n",
    "  parameters = get_sparql_dataframe(SPARQL_ENDPOINT, DATASET_PARAMS_QUERY.replace('{dataset_uri}', dataset['dataset']))\n",
    "  schema = []\n",
    "  for param_index, parameter in parameters.iterrows():\n",
    "    param = {}\n",
    "    param['bcodmo:name'] = parameter['supplied_name']\n",
    "    if parameter['supplied_definition'] is not None:\n",
    "      param['bcodmo:description'] = parameter['supplied_definition']\n",
    "    if parameter['datatype'] is not None:\n",
    "      param['bcodmo:datatype'] = parameter['datatype']\n",
    "    if parameter['units'] is not None:\n",
    "      param['bcodmo:units'] = parameter['units']\n",
    "    if parameter['format'] is not None:\n",
    "      param['bcodmo:valueFormat'] = parameter['format']\n",
    "    schema.append(param)\n",
    "\n",
    "  # Get the 'data' files for a Dataset (skip any supplemental documentation)\n",
    "  files = get_sparql_dataframe(SPARQL_ENDPOINT, DATASET_FILES_QUERY.replace('{dataset_uri}', dataset['dataset']))\n",
    "  for file_index, file in files.iterrows():\n",
    "\n",
    "    # Use Frictionless to describe the file\n",
    "    datafile = describe(file['url'])\n",
    "\n",
    "    # Get Table stats\n",
    "    if datafile.type == 'table':\n",
    "      datafile.infer(stats=True)\n",
    "\n",
    "    # Specify which dataset this file belongs to\n",
    "    datafile.sources = [source]\n",
    "\n",
    "    # If the file is marked as the primary file for the dataset, attach the parameters to the file\n",
    "    if schema is not None and file['is_primary_data_file'] == 'true':\n",
    "      datafile.custom['bcodmo:parameters'] = schema\n",
    "\n",
    "    # Add the file to the package\n",
    "    biosscope.add_resource(datafile)\n",
    "\n",
    "# Save the package\n",
    "print(biosscope.to_json())\n",
    "biosscope.to_json('datapackage.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdef19-fe01-412b-8b46-15ef0fdaf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
